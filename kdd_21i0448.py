# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hUNWfb_Fg3G2uOJBxWl83xlGlAAvkl7C

21i-0448


##Task 1.1: Loading the dataset
"""

# Install datasets library from Hugging Face
!pip install datasets
# Install Streamlit
!pip install streamlit
# Install NLTK for text preprocessing
!pip install nltk

"""### Task 1.2: Explore the dataset"""

import datasets

# Load the PubMed dataset from Hugging Face
dataset = datasets.load_dataset("scientific_papers", "pubmed")

# Explore the dataset
print(dataset)
print(dataset['train'][0])

# Check for missing values
print(dataset['train'].filter(lambda x: x['article'] == '' or x['abstract'] == ''))

"""### Task 1.3: Preprocess the dataset"""

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra whitespace
    text = text.strip()  # Remove leading and trailing whitespace
    text = text.lower()  # Lowercase text
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    tokens = word_tokenize(text)  # Tokenize text
    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
    return ' '.join(tokens)

# Apply preprocessing to the dataset
dataset = dataset.map(lambda x: {'article': preprocess_text(x['article']), 'abstract': preprocess_text(x['abstract'])})

"""(Optional question for personal research -  remember to del before submission)
why does it not run in one execution?
"""

import datasets

# Load PubMed dataset from Hugging Face
dataset = datasets.load_dataset("scientific_papers", "pubmed")

# Explore the dataset
print(dataset)
print(dataset['train'][0])

# Check for missing values
print(dataset['train'].filter(lambda x: x['article'] == '' or x['abstract'] == ''))
from datasets import load_dataset
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download NLTK data
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

# Preprocess text
def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra whitespace
    text = text.strip()  # Remove leading and trailing whitespace
    text = text.lower()  # Lowercase text
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    tokens = word_tokenize(text)  # Tokenize text
    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
    return ' '.join(tokens)

# preprocessing to the dataset
dataset = dataset.map(lambda x: {'article': preprocess_text(x['article']), 'abstract': preprocess_text(x['abstract'])})



"""I will be completely honest. I have yet to work on a web application project using Streamlit or Flask. I hope this opportunity would provide the best learning platform.
However, in the report is an approach how I would have approached this problem. (May have refered to online resources for guidance)
"""

